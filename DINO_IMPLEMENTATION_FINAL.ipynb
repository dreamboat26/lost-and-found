{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRGPfIucQBEc",
        "outputId": "3bb5f564-4a0b-4cb9-f2c5-1389d673e655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mildlyoverfitted'...\n",
            "remote: Enumerating objects: 356, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 356 (delta 71), reused 52 (delta 46), pack-reused 259\u001b[K\n",
            "Receiving objects: 100% (356/356), 815.38 KiB | 3.85 MiB/s, done.\n",
            "Resolving deltas: 100% (125/125), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jankrepl/mildlyoverfitted.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TanyaChutani/DINO_Tf2.x.git"
      ],
      "metadata": {
        "id": "bgDtHkSBgLOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "def compute_knn(backbone, data_loader_train, data_loader_val):\n",
        "    \"\"\"Get CLS embeddings and use KNN classifier on them.\n",
        "\n",
        "    We load all embeddings in memory and use sklearn. Should\n",
        "    be doable.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    backbone : timm.models.vision_transformer.VisionTransformer\n",
        "        Vision transformer whose head is just an identity\n",
        "        mapping.\n",
        "\n",
        "    data_loader_train, data_loader_val : torch.utils.data.DataLoader\n",
        "        Training and validation dataloader that does not apply any\n",
        "        augmentations. Just casting to tensor and then normalizing.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    val_accuracy : float\n",
        "        Validation accuracy.\n",
        "    \"\"\"\n",
        "    device = next(backbone.parameters()).device\n",
        "\n",
        "    data_loaders = {\n",
        "        \"train\": data_loader_train,\n",
        "        \"val\": data_loader_val,\n",
        "    }\n",
        "    lists = {\n",
        "        \"X_train\": [],\n",
        "        \"y_train\": [],\n",
        "        \"X_val\": [],\n",
        "        \"y_val\": [],\n",
        "    }\n",
        "\n",
        "    for name, data_loader in data_loaders.items():\n",
        "        for imgs, y in data_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            lists[f\"X_{name}\"].append(backbone(imgs).detach().cpu().numpy())\n",
        "            lists[f\"y_{name}\"].append(y.detach().cpu().numpy())\n",
        "\n",
        "    arrays = {k: np.concatenate(l) for k, l in lists.items()}\n",
        "\n",
        "    estimator = KNeighborsClassifier()\n",
        "    estimator.fit(arrays[\"X_train\"], arrays[\"y_train\"])\n",
        "    y_val_pred = estimator.predict(arrays[\"X_val\"])\n",
        "\n",
        "    acc = accuracy_score(arrays[\"y_val\"], y_val_pred)\n",
        "\n",
        "    return acc\n",
        "\n",
        "def compute_embedding(backbone, data_loader):\n",
        "    \"\"\"Compute CLS embedding and prepare for TensorBoard.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    backbone : timm.models.vision_transformer.VisionTransformer\n",
        "        Vision transformer. The head should be an identity mapping.\n",
        "\n",
        "    data_loader : torch.utils.data.DataLoader\n",
        "        Validation dataloader that does not apply any augmentations. Just\n",
        "        casting to tensor and then normalizing.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    embs : torch.Tensor\n",
        "        Embeddings of shape `(n_samples, out_dim)`.\n",
        "\n",
        "    imgs : torch.Tensor\n",
        "        Images of shape `(n_samples, 3, height, width)`.\n",
        "\n",
        "    labels : list\n",
        "        List of strings representing the classes.\n",
        "    \"\"\"\n",
        "    device = next(backbone.parameters()).device\n",
        "\n",
        "    embs_l = []\n",
        "    imgs_l = []\n",
        "    labels = []\n",
        "\n",
        "    for img, y in data_loader:\n",
        "        img = img.to(device)\n",
        "        embs_l.append(backbone(img).detach().cpu())\n",
        "        imgs_l.append(((img * 0.224) + 0.45).cpu())  # undo norm\n",
        "        labels.extend([data_loader.dataset.classes[i] for i in y.tolist()])\n",
        "\n",
        "    embs = torch.cat(embs_l, dim=0)\n",
        "    imgs = torch.cat(imgs_l, dim=0)\n",
        "\n",
        "    return embs, imgs, labels"
      ],
      "metadata": {
        "id": "Gd_jnD0wSvFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class DataAugmentation:\n",
        "    \"\"\"Create crops of an input image together with additional augmentation.\n",
        "\n",
        "    It generates 2 global crops and `n_local_crops` local crops.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    global_crops_scale : tuple\n",
        "        Range of sizes for the global crops.\n",
        "\n",
        "    local_crops_scale : tuple\n",
        "        Range of sizes for the local crops.\n",
        "\n",
        "    n_local_crops : int\n",
        "        Number of local crops to create.\n",
        "\n",
        "    size : int\n",
        "        The size of the final image.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    global_1, global_2 : transforms.Compose\n",
        "        Two global transforms.\n",
        "\n",
        "    local : transforms.Compose\n",
        "        Local transform. Note that the augmentation is stochastic so one\n",
        "        instance is enough and will lead to different crops.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        global_crops_scale=(0.4, 1),\n",
        "        local_crops_scale=(0.05, 0.4),\n",
        "        n_local_crops=8,\n",
        "        size=224,\n",
        "    ):\n",
        "        self.n_local_crops = n_local_crops\n",
        "        RandomGaussianBlur = lambda p: transforms.RandomApply(  # noqa\n",
        "            [transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2))],\n",
        "            p=p,\n",
        "        )\n",
        "\n",
        "        flip_and_jitter = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomApply(\n",
        "                    [\n",
        "                        transforms.ColorJitter(\n",
        "                            brightness=0.4,\n",
        "                            contrast=0.4,\n",
        "                            saturation=0.2,\n",
        "                            hue=0.1,\n",
        "                        ),\n",
        "                    ]\n",
        "                ),\n",
        "                transforms.RandomGrayscale(p=0.2),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        normalize = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.global_1 = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomResizedCrop(\n",
        "                    size,\n",
        "                    scale=global_crops_scale,\n",
        "                    interpolation=Image.BICUBIC,\n",
        "                ),\n",
        "                flip_and_jitter,\n",
        "                RandomGaussianBlur(1.0),  # always apply\n",
        "                normalize,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        self.global_2 = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomResizedCrop(\n",
        "                    size,\n",
        "                    scale=global_crops_scale,\n",
        "                    interpolation=Image.BICUBIC,\n",
        "                ),\n",
        "                flip_and_jitter,\n",
        "                RandomGaussianBlur(0.1),\n",
        "                transforms.RandomSolarize(170, p=0.2),\n",
        "                normalize,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        self.local = transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomResizedCrop(\n",
        "                    size,\n",
        "                    scale=local_crops_scale,\n",
        "                    interpolation=Image.BICUBIC,\n",
        "                ),\n",
        "                flip_and_jitter,\n",
        "                RandomGaussianBlur(0.5),\n",
        "                normalize,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"Apply transformation.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        img : PIL.Image\n",
        "            Input image.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        all_crops : list\n",
        "            List of `torch.Tensor` representing different views of\n",
        "            the input `img`.\n",
        "        \"\"\"\n",
        "        all_crops = []\n",
        "        all_crops.append(self.global_1(img))\n",
        "        all_crops.append(self.global_2(img))\n",
        "\n",
        "        all_crops.extend([self.local(img) for _ in range(self.n_local_crops)])\n",
        "\n",
        "        return all_crops\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"Network hooked up to the CLS token embedding.\n",
        "\n",
        "    Just a MLP with the last layer being normalized in a particular way.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_dim : int\n",
        "        The dimensionality of the token embedding.\n",
        "\n",
        "    out_dim : int\n",
        "        The dimensionality of the final layer (we compute the softmax over).\n",
        "\n",
        "    hidden_dim : int\n",
        "        Dimensionality of the hidden layers.\n",
        "\n",
        "    bottleneck_dim : int\n",
        "        Dimensionality of the second last layer.\n",
        "\n",
        "    n_layers : int\n",
        "        The number of layers.\n",
        "\n",
        "    norm_last_layer : bool\n",
        "        If True, then we freeze the norm of the weight of the last linear layer\n",
        "        to 1.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    mlp : nn.Sequential\n",
        "        Vanilla multi-layer perceptron.\n",
        "\n",
        "    last_layer : nn.Linear\n",
        "        Reparametrized linear layer with weight normalization. That means\n",
        "        that that it will have `weight_g` and `weight_v` as learnable\n",
        "        parameters instead of a single `weight`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim,\n",
        "        out_dim,\n",
        "        hidden_dim=512,\n",
        "        bottleneck_dim=256,\n",
        "        n_layers=3,\n",
        "        norm_last_layer=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if n_layers == 1:\n",
        "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
        "        else:\n",
        "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
        "            layers.append(nn.GELU())\n",
        "            for _ in range(n_layers - 2):\n",
        "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "                layers.append(nn.GELU())\n",
        "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
        "            self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self.last_layer = nn.utils.weight_norm(\n",
        "            nn.Linear(bottleneck_dim, out_dim, bias=False)\n",
        "        )\n",
        "        self.last_layer.weight_g.data.fill_(1)\n",
        "        if norm_last_layer:\n",
        "            self.last_layer.weight_g.requires_grad = False\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        \"\"\"Initialize learnable parameters.\"\"\"\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Of shape `(n_samples, in_dim)`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Of shape `(n_samples, out_dim)`.\n",
        "        \"\"\"\n",
        "        x = self.mlp(x)  # (n_samples, bottleneck_dim)\n",
        "        x = nn.functional.normalize(x, dim=-1, p=2)  # (n_samples, bottleneck_dim)\n",
        "        x = self.last_layer(x)  # (n_samples, out_dim)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiCropWrapper(nn.Module):\n",
        "    \"\"\"Convenience class for forward pass of multiple crops.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    backbone : timm.models.vision_transformer.VisionTransformer\n",
        "        Instantiated Vision Transformer. Note that we will take the `head`\n",
        "        attribute and replace it with `nn.Identity`.\n",
        "\n",
        "    new_head : Head\n",
        "        New head that is going to be put on top of the `backbone`.\n",
        "    \"\"\"\n",
        "    def __init__(self, backbone, new_head):\n",
        "        super().__init__()\n",
        "        backbone.head = nn.Identity()  # deactivate original head\n",
        "        self.backbone = backbone\n",
        "        self.new_head = new_head\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run the forward pass.\n",
        "\n",
        "        The different crops are concatenated along the batch dimension\n",
        "        and then a single forward pass is fun. The resulting tensor\n",
        "        is then chunked back to per crop tensors.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : list\n",
        "            List of `torch.Tensor` each of shape `(n_samples, 3, size, size)`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple\n",
        "            Tuple of `torch.Tensor` each of shape `(n_samples, out_dim)` where\n",
        "            `output_dim` is determined by `Head`.\n",
        "        \"\"\"\n",
        "        n_crops = len(x)\n",
        "        concatenated = torch.cat(x, dim=0)  # (n_samples * n_crops, 3, size, size)\n",
        "        cls_embedding = self.backbone(concatenated)  # (n_samples * n_crops, in_dim)\n",
        "        logits = self.new_head(cls_embedding)  # (n_samples * n_crops, out_dim)\n",
        "        chunks = logits.chunk(n_crops)  # n_crops * (n_samples, out_dim)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    \"\"\"The loss function.\n",
        "\n",
        "    We subclass the `nn.Module` becuase we want to create a buffer for the\n",
        "    logits center of the teacher.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    out_dim : int\n",
        "        The dimensionality of the final layer (we computed the softmax over).\n",
        "\n",
        "    teacher_temp, student_temp : float\n",
        "        Softmax temperature of the teacher resp. student.\n",
        "\n",
        "    center_momentum : float\n",
        "        Hyperparameter for the exponential moving average that determines\n",
        "        the center logits. The higher the more the running average matters.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, out_dim, teacher_temp=0.04, student_temp=0.1, center_momentum=0.9\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.student_temp = student_temp\n",
        "        self.teacher_temp = teacher_temp\n",
        "        self.center_momentum = center_momentum\n",
        "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
        "\n",
        "    def forward(self, student_output, teacher_output):\n",
        "        \"\"\"Evaluate loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        student_output, teacher_output : tuple\n",
        "            Tuple of tensors of shape `(n_samples, out_dim)` representing\n",
        "            logits. The length is equal to number of crops.\n",
        "            Note that student processed all crops and that the two initial crops\n",
        "            are the global ones.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss : torch.Tensor\n",
        "            Scalar representing the average loss.\n",
        "        \"\"\"\n",
        "        student_temp = [s / self.student_temp for s in student_output]\n",
        "        teacher_temp = [(t - self.center) / self.teacher_temp for t in teacher_output]\n",
        "\n",
        "        student_sm = [F.log_softmax(s, dim=-1) for s in student_temp]\n",
        "        teacher_sm = [F.softmax(t, dim=-1).detach() for t in teacher_temp]\n",
        "\n",
        "        total_loss = 0\n",
        "        n_loss_terms = 0\n",
        "\n",
        "        for t_ix, t in enumerate(teacher_sm):\n",
        "            for s_ix, s in enumerate(student_sm):\n",
        "                if t_ix == s_ix:\n",
        "                    continue\n",
        "\n",
        "                loss = torch.sum(-t * s, dim=-1)  # (n_samples,)\n",
        "                total_loss += loss.mean()  # scalar\n",
        "                n_loss_terms += 1\n",
        "\n",
        "        total_loss /= n_loss_terms\n",
        "        self.update_center(teacher_output)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_center(self, teacher_output):\n",
        "        \"\"\"Update center used for teacher output.\n",
        "\n",
        "        Compute the exponential moving average.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        teacher_output : tuple\n",
        "            Tuple of tensors of shape `(n_samples, out_dim)` where each\n",
        "            tensor represents a different crop.\n",
        "        \"\"\"\n",
        "        batch_center = torch.cat(teacher_output).mean(\n",
        "            dim=0, keepdim=True\n",
        "        )  # (1, out_dim)\n",
        "        self.center = self.center * self.center_momentum + batch_center * (\n",
        "            1 - self.center_momentum\n",
        "        )\n",
        "\n",
        "def clip_gradients(model, clip=2.0):\n",
        "    \"\"\"Rescale norm of computed gradients.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Module.\n",
        "\n",
        "    clip : float\n",
        "        Maximum norm.\n",
        "    \"\"\"\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            clip_coef = clip / (param_norm + 1e-6)\n",
        "            if clip_coef < 1:\n",
        "                p.grad.data.mul_(clip_coef)"
      ],
      "metadata": {
        "id": "lV-qL9gFTQx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RgrRkJWTcO9",
        "outputId": "7b8f682f-f7d4-43ef-faf0-c3b93eb00807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class DataAugmentationDino:\n",
        "    def __init__(\n",
        "        self,\n",
        "        global_crops_scale,\n",
        "        local_crops_scale,\n",
        "        local_crops_number,\n",
        "        global_image_size=[224, 224],\n",
        "        local_image_size=[96, 96],\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std_dev=[0.229, 0.224, 0.225],\n",
        "    ):\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_dev\n",
        "        self.local_image_size = local_image_size\n",
        "        self.global_image_size = global_image_size\n",
        "        self.local_crops_scale = local_crops_scale\n",
        "        self.local_crops_number = local_crops_number\n",
        "        self.global_crops_scale = global_crops_scale\n",
        "\n",
        "        self.flip_aug = tf.keras.Sequential(\n",
        "            [tf.keras.layers.RandomFlip(mode=\"horizontal\")]\n",
        "        )\n",
        "\n",
        "    def _standardize_normalize(self, image):\n",
        "        image = image / 255.0\n",
        "        image -= self.mean\n",
        "        image /= self.std_dev\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        return image\n",
        "\n",
        "    def _color_jitter(image):\n",
        "        image = tf.image.random_brightness(image, max_delta=0.4)\n",
        "        image = tf.image.random_contrast(image, lower=0.0, upper=0.4)\n",
        "        image = tf.image.random_saturation(image, lower=0.0, upper=0.2)\n",
        "        image = tf.image.random_hue(image, max_delta=0.1)\n",
        "        return image\n",
        "\n",
        "    def _crop_resize(self, image, mode=\"global\"):\n",
        "        scalee = self.global_crops_scale if mode == \"global\" else self.local_crops_scale\n",
        "        final_size = (\n",
        "            self.global_image_size if mode == \"global\" else self.local_image_size\n",
        "        )\n",
        "        height, width, channels = tf.shape(image)\n",
        "        scaling_hw = tf.cast(tf.concat([height, width], axis=0), tf.float32)\n",
        "        scale = tf.multiply(scalee, scaling_hw)\n",
        "        scale = (\n",
        "            tf.cast(scale[0].numpy(), tf.int32),\n",
        "            tf.cast(scale[1].numpy(), tf.int32),\n",
        "            channels,\n",
        "        )\n",
        "        image = tf.image.random_crop(value=image, size=scale)\n",
        "        image = tf.image.resize(image, final_size, method=\"bicubic\")\n",
        "        return image\n",
        "\n",
        "    def _apply_aug(self, image, mode=\"global\"):\n",
        "        image = self.flip_aug(image)\n",
        "        image = self._crop_resize(image, mode)\n",
        "        image = self._standardize_normalize(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __call__(self, image):\n",
        "        crops = []\n",
        "        crops.append(self._apply_aug(image))\n",
        "        crops.append(self._apply_aug(image))\n",
        "        for _ in range(self.local_crops_number):\n",
        "            crops.append(self._apply_aug(image, mode=\"local\"))\n",
        "        return crops"
      ],
      "metadata": {
        "id": "r5GDH5eza-_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mode,\n",
        "        batch_size,\n",
        "        dataset_path,\n",
        "        local_image_size,\n",
        "        global_image_size,\n",
        "        shuffle=True,\n",
        "    ):\n",
        "        self.mode = mode\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset_path = dataset_path\n",
        "        self.dataset = os.listdir(dataset_path)\n",
        "        self.local_image_size = local_image_size\n",
        "        self.global_image_size = global_image_size\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def _load_image(self, path, data_augmentation):\n",
        "        image = tf.io.read_file(path)\n",
        "        image = tf.image.decode_image(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = data_augmentation(image) if self.mode == \"train\" else image\n",
        "        return image\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.index = tf.range(len(self.dataset))\n",
        "        if self.shuffle == True:\n",
        "            tf.random.shuffle(self.index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indexes = self.index[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        datset_keys = [self.dataset[k] for k in indexes]\n",
        "        (global_images, local_images) = self.__data_generation(datset_keys)\n",
        "        return global_images, local_images\n",
        "\n",
        "    def __data_generation(self, index):\n",
        "        batch_global, batch_local = [], []\n",
        "        dino = DataAugmentationDino((0.4, 1.0), (0.05, 0.4), 8)\n",
        "        for idx, i in enumerate(index):\n",
        "            images = self._load_image(os.path.join(self.dataset_path, i), dino)\n",
        "            global_images = images[:2]\n",
        "            # unable to stack varied size input in the dataset\n",
        "            local_images = images[2:]\n",
        "            batch_local.append(local_images)\n",
        "            batch_global.append(global_images)\n",
        "        return batch_global, batch_local"
      ],
      "metadata": {
        "id": "hlZZgeLibPCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class TeacherTemp(tf.keras.callbacks.Callback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temp,\n",
        "        nepochs=100,\n",
        "        teacher_temp=0.04,\n",
        "        warmup_teacher_temp=0.04,\n",
        "        warmup_teacher_temp_epochs=30,\n",
        "    ):\n",
        "        super(TeacherTemp, self).__init__()\n",
        "        self.temp = temp\n",
        "        self.teacher_temp_schedule = tf.concat(\n",
        "            (\n",
        "                tf.linspace(\n",
        "                    warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs\n",
        "                ),\n",
        "                tf.ones((nepochs - warmup_teacher_temp_epochs)) * teacher_temp,\n",
        "            ),\n",
        "            axis=0,\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.temp = tf.Variable(self.temp, trainable=True, dtype=tf.float32)\n",
        "        tf.keras.backend.set_value(self.temp, self.teacher_temp_schedule[epoch])\n",
        "        logs[\"temp\"] = tf.keras.backend.get_value(self.temp)"
      ],
      "metadata": {
        "id": "Hn3rNZycbWdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class DinoLoss(tf.keras.losses.Loss):\n",
        "    def __init__(\n",
        "        self,\n",
        "        nepochs=100,\n",
        "        out_dim=65536,\n",
        "        ncrops=2,\n",
        "        warmup_teacher_temp=0.04,\n",
        "        teacher_temp=0.04,\n",
        "        warmup_teacher_temp_epochs=30,\n",
        "        student_temp=0.1,\n",
        "        center_momentum=0.9,\n",
        "    ):\n",
        "        super(DinoLoss, self).__init__()\n",
        "        self.ncrops = ncrops\n",
        "        self.student_temp = student_temp\n",
        "        self.center_momentum = center_momentum\n",
        "\n",
        "        self.teacher_temp_schedule = tf.concat(\n",
        "            (\n",
        "                tf.linspace(\n",
        "                    warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs\n",
        "                ),\n",
        "                tf.ones((nepochs - warmup_teacher_temp_epochs)) * teacher_temp,\n",
        "            ),\n",
        "            axis=0,\n",
        "        )\n",
        "\n",
        "    def update_center(self, teacher_output):\n",
        "        batch_center = tf.math.reduce_sum(teacher_output, axis=0)\n",
        "        batch_center = batch_center / tf.cast(len(teacher_output), tf.float32)\n",
        "        self.center = tf.stop_gradient(\n",
        "            self.center * self.center_momentum\n",
        "            + batch_center * (1 - self.center_momentum)\n",
        "        )\n",
        "\n",
        "    def call(self, student_output, teacher_output):\n",
        "        teacher_output = tf.cast(teacher_output, tf.float32)\n",
        "        student_output = tf.cast(student_output, tf.float32)\n",
        "\n",
        "        student_out = student_output / self.student_temp\n",
        "        student_out = tf.split(student_out, num_or_size_splits=self.ncrops)\n",
        "\n",
        "        self.center = tf.zeros_like(teacher_output, dtype=tf.float32)\n",
        "        teacher_out = tf.stop_gradient(\n",
        "            tf.nn.softmax(\n",
        "                (teacher_output - self.center) / TeacherTemp(0.04).temp, axis=-1\n",
        "            )\n",
        "        )\n",
        "        teacher_out = tf.split(\n",
        "            tf.tile(teacher_out, tf.constant([2, 1], tf.int32)), num_or_size_splits=1\n",
        "        )\n",
        "\n",
        "        total_loss = 0\n",
        "        n_loss_terms = 0\n",
        "        for idx, q in enumerate(teacher_out):\n",
        "            for v in range(len(student_out)):\n",
        "                q = tf.stop_gradient(q)\n",
        "                if v == idx:\n",
        "                    continue\n",
        "                loss = tf.reduce_sum(\n",
        "                    -q * tf.nn.log_softmax(student_out[v], axis=-1), axis=-1\n",
        "                )\n",
        "                total_loss += tf.math.reduce_mean(loss)\n",
        "                n_loss_terms += 1\n",
        "        total_loss /= n_loss_terms\n",
        "        self.update_center(teacher_output)\n",
        "        return total_loss"
      ],
      "metadata": {
        "id": "TBr3pRBcbZRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vit_keras\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMYj_WANbiFV",
        "outputId": "0f54a9d2-8ec4-4a01-ebe4-69baf82bf2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vit_keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit_keras) (1.11.4)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit_keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit_keras) (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg1MW9Brbwwz",
        "outputId": "bbef4957-cd9e-44b9-f7e9-2e603a4e0d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from vit_keras import vit, utils\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "class MultiCropWrapper(tf.keras.models.Model):\n",
        "    def __init__(self, backbone, head, weights=None):\n",
        "        super(MultiCropWrapper, self).__init__()\n",
        "        self.head = head\n",
        "        self.backbone = backbone\n",
        "        if weights:\n",
        "            try:\n",
        "                print(\"Restoring model weights from: \", weights)\n",
        "                self.load_weights(weights)\n",
        "            except Exception:\n",
        "                raise ValueError\n",
        "\n",
        "    @staticmethod\n",
        "    def unique_consecutive(x):\n",
        "        neq = tf.math.not_equal(x, x)\n",
        "        neq = tf.cast(neq, tf.int32)\n",
        "        if neq.shape[0] > 1:\n",
        "            neq = tf.math.cumsum(tf.cast(neq, tf.int32), axis=0)\n",
        "        neq = tf.concat([[0], neq], axis=0)\n",
        "        _, _, count = tf.unique_with_counts(neq)\n",
        "        return count\n",
        "\n",
        "    def call(self, x):\n",
        "        if not isinstance(x, list):\n",
        "            x = [x]\n",
        "        unq = tf.constant([inp.shape[0] for inp in x], dtype=tf.int32)\n",
        "        count = self.unique_consecutive(unq)\n",
        "        start_idx, output = tf.constant(0), tf.zeros((0, 768), dtype=tf.float32)\n",
        "        for end_idx in count:\n",
        "            tf.autograph.experimental.set_loop_options(\n",
        "                shape_invariants=[(output, tf.TensorShape([None, None]))]\n",
        "            )\n",
        "            _out = self.backbone(\n",
        "                x[tf.get_static_value(start_idx) : tf.get_static_value(end_idx)]\n",
        "            )\n",
        "            if isinstance(_out, tuple):\n",
        "                _out = _out[0]\n",
        "            output = tf.concat([output, _out], axis=0)\n",
        "            start_idx = end_idx\n",
        "        return self.head(output)\n",
        "\n",
        "\n",
        "def load_base(image_size, include_pretrained=True):\n",
        "    model = vit.vit_b16(\n",
        "        image_size=image_size,\n",
        "        pretrained=include_pretrained,\n",
        "        pretrained_top=False,\n",
        "        include_top=False,\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ZC9pmjbfxR",
        "outputId": "165b7705-606a-42f7-c530-73b0d961dedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "\n",
        "class DinoHead(tf.keras.models.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim=768,\n",
        "        out_dim=65536,\n",
        "        use_bn=False,\n",
        "        norm_last_layer=True,\n",
        "        nlayers=3,\n",
        "        hidden_dim=2048,\n",
        "        bottleneck_dim=256,\n",
        "    ):\n",
        "        super(DinoHead, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.use_bn = use_bn\n",
        "        self.out_dim = out_dim\n",
        "        self.nlayers = nlayers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bottleneck_dim = bottleneck_dim\n",
        "        self.norm_last_layer = norm_last_layer\n",
        "        self.last_layer = tf.keras.layers.Dense(self.out_dim)\n",
        "\n",
        "        self.mlp_block = self.mlp()\n",
        "\n",
        "    def mlp(self):\n",
        "        layer = []\n",
        "        layer.append(tf.keras.layers.Dense(self.hidden_dim, input_shape=(self.in_dim,)))\n",
        "        if self.use_bn:\n",
        "            layer.append(tf.keras.layers.BatchNormalization())\n",
        "        layer.append(tfa.layers.GELU())\n",
        "        for _ in range(self.nlayers - 2):\n",
        "            layer.append(tf.keras.layers.Dense(self.hidden_dim))\n",
        "        if self.use_bn:\n",
        "            layer.append(tf.keras.layers.BatchNormalization())\n",
        "        layer.append(tfa.layers.GELU())\n",
        "        layer.append(tf.keras.layers.Dense(self.bottleneck_dim))\n",
        "        return tf.keras.Sequential(layer)\n",
        "\n",
        "    def call(self, input_tensor, training=None):\n",
        "        x = self.mlp_block(input_tensor, training)\n",
        "        x = tf.nn.l2_normalize(x, axis=-1)\n",
        "        x = self.last_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xCCn61XqcP62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Dino(tf.keras.models.Model):\n",
        "    def __init__(\n",
        "        self, teacher_model, student_model, student_weights=None, teacher_weights=None\n",
        "    ):\n",
        "        super(Dino, self).__init__()\n",
        "        self.teacher_model = teacher_model\n",
        "        self.student_model = student_model\n",
        "        self.student_weights = student_weights\n",
        "        self.teacher_weights = teacher_weights\n",
        "        self.dino_loss = DinoLoss()\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(Dino, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def train_step(self, data):\n",
        "        global_image, local_image = data\n",
        "        local_image = sum(local_image, ())\n",
        "        global_image = sum(global_image, ())\n",
        "        local_image = tf.stack(local_image)\n",
        "        global_image = tf.stack(global_image)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            teacher_output = self.teacher_model(global_image)\n",
        "            student_output = self.student_model(local_image)\n",
        "            loss = tf.reduce_mean(self.dino_loss(student_output, teacher_output))\n",
        "            student_gradients = tape.gradient(\n",
        "                loss, self.student_model.trainable_variables\n",
        "            )\n",
        "            self.optimizer.apply_gradients(\n",
        "                zip(student_gradients, self.student_model.trainable_variables)\n",
        "            )\n",
        "            return {\"loss\": loss}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        global_image, local_image = data\n",
        "\n",
        "        local_image = sum(local_image, ())\n",
        "        global_image = sum(global_image, ())\n",
        "        local_image = tf.stack(local_image)\n",
        "        global_image = tf.stack(global_image)\n",
        "\n",
        "        teacher_output = self.teacher_model(global_image, training=False)\n",
        "        student_output = self.student_model(local_image, training=False)\n",
        "\n",
        "        loss = tf.reduce_mean(self.dino_loss(student_output, teacher_output))\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def call(self, image):\n",
        "        output = self.teacher_model(image, training=False)"
      ],
      "metadata": {
        "id": "3JIGKfpTcVIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import tensorflow as tf\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-epoch\", \"--epochs\", type=int, metavar=\"\", default=100)\n",
        "    parser.add_argument(\"-b\", \"--batch_size\", type=int, metavar=\"\", default=2)\n",
        "    parser.add_argument(\"-ct\", \"--crop_teacher\", type=int, metavar=\"\", default=224)\n",
        "    parser.add_argument(\"-cs\", \"--crop_student\", type=int, metavar=\"\", default=96)\n",
        "    parser.add_argument(\n",
        "        \"-d_train\",\n",
        "        \"--dataset_train\",\n",
        "        type=str,\n",
        "        metavar=\"\",\n",
        "        default=\"VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-d_test\",\n",
        "        \"--dataset_test\",\n",
        "        type=str,\n",
        "        metavar=\"\",\n",
        "        default=\"VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-s_weights\",\n",
        "        \"--student_weights_path\",\n",
        "        type=str,\n",
        "        metavar=\"\",\n",
        "        default=\"student_weights\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"-t_weights\",\n",
        "        \"--teacher_weights_path\",\n",
        "        type=str,\n",
        "        metavar=\"\",\n",
        "        default=\"teacher_weights\",\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    head = DinoHead()\n",
        "\n",
        "    student = load_base(args.crop_student)\n",
        "    teacher = load_base(args.crop_teacher)\n",
        "\n",
        "    student = MultiCropWrapper(backbone=student, head=head)\n",
        "    teacher = MultiCropWrapper(backbone=teacher, head=head)\n",
        "\n",
        "    model = Dino(teacher, student)\n",
        "\n",
        "    train_dataset = DataGenerator(\n",
        "        mode=\"train\",\n",
        "        dataset_path=args.dataset_train,\n",
        "        batch_size=args.batch_size,\n",
        "        local_image_size=args.crop_student,\n",
        "        global_image_size=args.crop_teacher,\n",
        "    )\n",
        "\n",
        "    val_dataset = DataGenerator(\n",
        "        mode=\"val\",\n",
        "        dataset_path=args.dataset_test,\n",
        "        batch_size=args.batch_size,\n",
        "        local_image_size=args.crop_student,\n",
        "        global_image_size=args.crop_teacher,\n",
        "    )\n",
        "\n",
        "    learning_rate = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "        boundaries=[args.epochs / 2], values=[0.0001, 0.00001]\n",
        "    )\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            args.teacher_weights_path,\n",
        "            monitor=\"loss\",\n",
        "            save_best_only=True,\n",
        "            save_weights_only=False,\n",
        "            mode=\"auto\",\n",
        "        )\n",
        "    ]\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
        "    model.build(input_shape=(1, args.crop_teacher, args.crop_teacher, 3))\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=args.epochs,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "    model.student_model.save_weights(args.student_weights_path)\n",
        "    model.teacher_model.save_weights(args.teacher_weights_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "qmGqUP57caJc",
        "outputId": "67d0fa6a-12a6-44ea-a561-addfcba6e7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [-epoch] [-b] [-ct] [-cs] [-d_train] [-d_test] [-s_weights]\n",
            "                                [-t_weights]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-93d5cfa8-e07a-4773-ba1c-f63c6588b3a3.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}