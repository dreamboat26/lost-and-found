{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72g9lk8nEdaa"
      },
      "source": [
        "# **Jupyter Notebook: Zephyr-7B-Beta LLM Fine-Tuning with Predibase on Gridspace-Stanford Harper Valley (GSHV) Dataset**\n",
        "\n",
        "This quickstart will show you how to prompt, fine-tune, and deploy LLMs in Predibase. We'll be following a code generation use case where our end result will be a fine-tuned Zephyr-7B-Beta model that takes in voice call text transcripts as input and returns Task Type as output."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U predibase\n",
        "! pip install -U pprint\n",
        "! pip install -U tqdm"
      ],
      "metadata": {
        "id": "ykQMuAtzadJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkGcwkRZBWTl"
      },
      "source": [
        "# **Setup**\n",
        "\n",
        "You'll first need to initialize your PredibaseClient object and configure your API token.\n",
        "\n",
        "To get your Predibase API token please sign up to a free trial at [predibase.com](https://predibase.com/free-trial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POjTWL2EArrT"
      },
      "outputs": [],
      "source": [
        "from predibase import PredibaseClient\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BZxHsx9L8_b"
      },
      "outputs": [],
      "source": [
        "my_api_token: str = \"<Your token here>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OATZF70DMpyK"
      },
      "outputs": [],
      "source": [
        "pc: PredibaseClient = PredibaseClient(token=my_api_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fN3GBT4zxz7S"
      },
      "outputs": [],
      "source": [
        "pc.list_llm_deployments(active_only=True, print_as_table=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AicnArh2CNgI"
      },
      "source": [
        "# **Prompt a deployed LLM**\n",
        "\n",
        "For our code generation use case, let's first see how Llama 2 7B performs out of the box.\n",
        "\n",
        "The first line is where we specify which deployed LLM we intend to query. If you are in the Predibase SaaS environment, you have a few shared LLMs available to you, including Llama 2 7B. If you are in a VPC environment, you'll need to deploy an LLM before you can query it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq80chaTxz7T"
      },
      "outputs": [],
      "source": [
        "from predibase.resource.llm.interface import HuggingFaceLLM, LLMDeployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RMXGhynxz7T"
      },
      "outputs": [],
      "source": [
        "from predibase.pql.api import ServerResponseError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpXpUSYYxz7T"
      },
      "outputs": [],
      "source": [
        "pb_model_deployment_name: str = \"zephyr-7b-beta\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqm3VgzUxz7U"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    llm = pc.LLM(uri=\"hf://HuggingFaceH4/zephyr-7b-beta\")\n",
        "    base_llm_deployment = llm.deploy(deployment_name=pb_model_deployment_name, engine_template=\"llm-gpu-small\").get()\n",
        "except ServerResponseError:\n",
        "    print(f'\\n[WARNING] DEPLOYMENT_EXISTS:\\n{pb_model_deployment_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EEHVvrcMRe6"
      },
      "outputs": [],
      "source": [
        "base_llm_deployment = pc.LLM(uri=f\"pb://deployments/{pb_model_deployment_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13EjBilDxz7U"
      },
      "outputs": [],
      "source": [
        "assert base_llm_deployment.name == pb_model_deployment_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaUDs39txz7U"
      },
      "outputs": [],
      "source": [
        "base_llm_deployment.wait_for_ready()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0bkamAGxz7U"
      },
      "outputs": [],
      "source": [
        "print(base_llm_deployment.default_prompt_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-gVp6Q4xz7U"
      },
      "outputs": [],
      "source": [
        "# Define the template used to prompt the model for each example\n",
        "# Note the 4-space indentation, which is necessary for the YAML templating.\n",
        "prompt_template: str = \"\"\"\n",
        "    Consider the case of a customer contacting the support center.\n",
        "    The term \"task type\" refers to the reason for why the customer contacted support.\n",
        "\n",
        "    ### The possible task types are: ###\n",
        "    - replace card\n",
        "    - transfer money\n",
        "    - check balance\n",
        "    - order checks\n",
        "    - pay bill\n",
        "    - reset password\n",
        "    - schedule appointment\n",
        "    - get branch hours\n",
        "    - none of the above\n",
        "\n",
        "    Summarize the issue/question/reason that drove the customer to contact support:\n",
        "\n",
        "    ### Transcript: {transcript}\n",
        "\n",
        "    ### Task Type:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzwc70Rfxz7U"
      },
      "outputs": [],
      "source": [
        "target: str = \"task_type\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NbywkoExz7U"
      },
      "outputs": [],
      "source": [
        "config: dict = llm.get_finetune_templates().default.to_config(prompt_template=prompt_template, target=target)\n",
        "pprint(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjHejjwPxz7V"
      },
      "outputs": [],
      "source": [
        "test_transcript: str = \"\"\"\n",
        "<caller> hello <agent> hello this is [unintelligible] national bank my name is jennifer <agent> how can i help you today <caller> hi my name is james william <caller> i lost my debit card <caller> can you send me a new one <agent> yes <agent> uh which card or would you like to replace <caller> my debit card <agent> okay i've ordered your replacement debit card is there anything else i can help you with today <caller> no that's gonna be all for me today <agent> [noise] <agent> alright thank you for calling have a great day <caller> you too bye <agent> [noise] <agent> [noise]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LquAkmghxz7V"
      },
      "outputs": [],
      "source": [
        "test_prompt: str = prompt_template.format(**{\"transcript\": test_transcript})\n",
        "print(test_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-UT7wPA4Q75d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRP93WAYCM5C"
      },
      "outputs": [],
      "source": [
        "result = base_llm_deployment.prompt(\n",
        "    data=test_prompt,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    bypass_system_prompt=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjAParnAxz7Z"
      },
      "outputs": [],
      "source": [
        "print(f'\\n[GENERATED_TEXT] BASE_MODEL_PREDICTION:\\n{result.generated_text} ; TYPE: {str(type(result.generated_text))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li4VqIw4CVp2"
      },
      "source": [
        "# **Fine-tune a pretrained LLM**\n",
        "\n",
        "Next we'll upload a dataset and fine-tune to see if we can get better performance.\n",
        "\n",
        "The [Gridspace-Stanford Harper Valley (GSHV)](https://github.com/cricketclub/gridspace-stanford-harper-valley) dataset is used for fine-tuning large language models to analyze transcribed customer service voice calls to produce reasons for contact (\"task type\", or \"contact reason\") and consists of the following columns:\n",
        "\n",
        "- `transcript` that contains the conversation between the caller and the agent\n",
        "- the discerned `task_type`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocess the Dataset**\n",
        "\n",
        "This flow assumes that you have copied the dataset into your GDrive and mounted it to a location under `/content/drive/MyDrive/GridspaceStanfordHarperValley`. The original dataset can be found [here.](https://github.com/cricketclub/gridspace-stanford-harper-valley/)"
      ],
      "metadata": {
        "id": "_9L-_Sn_ZFQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN8OfmZyxz7Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pwIL7owRaWbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/drive/MyDrive/GridspaceStanfordHarperValley')"
      ],
      "metadata": {
        "id": "wBWw92crfINO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ53RIb2xz7Z"
      },
      "outputs": [],
      "source": [
        "transcript_files_path: str = \"/content/drive/MyDrive/GridspaceStanfordHarperValley/data/transcript\"\n",
        "metadata_files_path: str = \"/content/drive/MyDrive/GridspaceStanfordHarperValley/data/metadata\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moQMMd5sxz7a"
      },
      "outputs": [],
      "source": [
        "file_names: list[str] = os.listdir(transcript_files_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HqALvhPxz7a"
      },
      "outputs": [],
      "source": [
        "len(file_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "EXNOfB5Ei5NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muRTFDkQ83D8"
      },
      "outputs": [],
      "source": [
        "all_transcripts = []\n",
        "all_task_types = []\n",
        "for file_name in tqdm.tqdm(file_names):\n",
        "    metadata_file_path = os.path.join(metadata_files_path, file_name)\n",
        "    try:\n",
        "        with open(metadata_file_path, 'r') as metadata_file:\n",
        "            metadata = json.load(metadata_file)\n",
        "            task_type = metadata[\"tasks\"][0][\"task_type\"]\n",
        "            all_task_types.append(task_type)\n",
        "    except FileNotFoundError:\n",
        "        continue\n",
        "\n",
        "    transcript_file_path = os.path.join(transcript_files_path, file_name)\n",
        "    with open(transcript_file_path, 'r') as transcript_file:\n",
        "        conversation_turns = json.load(transcript_file)\n",
        "\n",
        "        transcript_part_list = []\n",
        "        for turn_item in conversation_turns:\n",
        "          transcript_part = f'<{turn_item[\"speaker_role\"]}>' + \" \" + turn_item[\"human_transcript\"]\n",
        "          transcript_part_list.append(transcript_part)\n",
        "\n",
        "        transcript_text = \" \".join(transcript_part for transcript_part in transcript_part_list)\n",
        "        all_transcripts.append(transcript_text)\n",
        "print(len(all_transcripts), len(all_task_types))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YOwGZ59N11C"
      },
      "outputs": [],
      "source": [
        "raw_data: dict = {\"transcript\": all_transcripts, \"task_type\": all_task_types}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyjbzaUNDluT"
      },
      "outputs": [],
      "source": [
        "df_dataset_original: pd.DataFrame = pd.DataFrame(data=raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTOeGIS8Nic5"
      },
      "outputs": [],
      "source": [
        "df_train: pd.DataFrame = df_dataset_original.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOCi05waDlxQ"
      },
      "outputs": [],
      "source": [
        "df_evaluation: pd.DataFrame = df_train.sample(n=10, random_state=200)\n",
        "df_train = df_train.drop(df_evaluation.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwmqZzDVmRsa"
      },
      "outputs": [],
      "source": [
        "df_test = df_train.sample(n=200, random_state=200)\n",
        "df_train = df_train.drop(df_test.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbtuNVyAmpOC"
      },
      "outputs": [],
      "source": [
        "df_validation = df_train.sample(n=100, random_state=200)\n",
        "df_train = df_train.drop(df_validation.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VAG6CQIq6oK"
      },
      "outputs": [],
      "source": [
        "assert df_train.shape[0] == 700\n",
        "assert df_test.shape[0] == 200\n",
        "assert df_validation.shape[0] == 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XjPa4XCq6dv"
      },
      "outputs": [],
      "source": [
        "df_train[\"split\"] = np.zeros(df_train.shape[0])\n",
        "df_test[\"split\"] = np.ones(df_test.shape[0])\n",
        "df_validation[\"split\"] = np.full(df_validation.shape[0], 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwjCzdMsq6RC"
      },
      "outputs": [],
      "source": [
        "df_dataset = pd.concat([df_train, df_test, df_validation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0yh3pl3piVJ"
      },
      "outputs": [],
      "source": [
        "df_dataset[\"split\"] = df_dataset[\"split\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHV-Gty_s1sA"
      },
      "outputs": [],
      "source": [
        "df_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjj5g2bOYpDV"
      },
      "outputs": [],
      "source": [
        "assert df_dataset[df_dataset[\"split\"] == 0].shape[0] == 700\n",
        "assert df_dataset[df_dataset[\"split\"] == 1].shape[0] == 200\n",
        "assert df_dataset[df_dataset[\"split\"] == 2].shape[0] == 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YlNOIOSlzNM"
      },
      "outputs": [],
      "source": [
        "df_dataset.head(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3x3stdDKoCT"
      },
      "outputs": [],
      "source": [
        "# Calculating the length of each cell in each column\n",
        "df_dataset['num_characters_transcript'] = df_dataset['transcript'].apply(lambda x: len(x))\n",
        "df_dataset['num_characters_task_type'] = df_dataset['task_type'].apply(lambda x: len(x))\n",
        "\n",
        "# Show Distribution\n",
        "df_dataset.hist(column=['num_characters_transcript', 'num_characters_task_type'])\n",
        "\n",
        "# Calculating the average\n",
        "average_chars_transcript = df_dataset['num_characters_transcript'].mean()\n",
        "average_chars_task_type = df_dataset['num_characters_task_type'].mean()\n",
        "\n",
        "print(f'Average number of tokens in the transcript column: {(average_chars_transcript / 3):.0f}')\n",
        "print(f'Average number of tokens in the task_type column: {(average_chars_task_type / 3):.0f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7WejJVkP-tF"
      },
      "outputs": [],
      "source": [
        "df_evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDMT0IdfWmIN"
      },
      "source": [
        "## **Now we will perform the following actions to start our fine-tuning job:**\n",
        "1. Upload the dataset to Predibase for training\n",
        "2. Create a prompt template to use for fine-tuning\n",
        "3. Select the LLM we want to fine-tune\n",
        "4. Kick off the fine-tuning job\n",
        "\n",
        "The fine-tuning job should take around 35-45 minutes total. Queueing time depends on how quickly we're able acquire resources and what other jobs might be ahead in the queue. The training time itself should be around 25-30 minutes. As the model trains, you can receive updated metrics in your notebook or terminal. You can also see metrics and visualizations in the Predibase UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38YtYUoXwUhK"
      },
      "outputs": [],
      "source": [
        "from predibase.resource.dataset import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8WVXUPvxz7c"
      },
      "outputs": [],
      "source": [
        "dataset_name: str = \"gridspace_stanford_harper_valley\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO2scZ9kxz7c"
      },
      "outputs": [],
      "source": [
        "dataset: Dataset = pc.create_dataset_from_df(df=df_dataset, name=dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKAPXD_VCbNd"
      },
      "outputs": [],
      "source": [
        "llm = pc.LLM(uri=\"hf://HuggingFaceH4/zephyr-7b-beta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uVTGrooTadL"
      },
      "outputs": [],
      "source": [
        "from predibase.resource.model import ModelFuture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWBi8zf5Slue"
      },
      "outputs": [],
      "source": [
        "# Default argument values are commented out.\n",
        "job: ModelFuture = llm.finetune(\n",
        "    prompt_template=prompt_template,\n",
        "    target=target,\n",
        "    dataset=dataset,\n",
        "    # engine=engine,\n",
        "    # config=None,\n",
        "    # repo=\"optional-custom-model-repository-name\",\n",
        "    epochs=5,\n",
        "    # train_steps=None,\n",
        "    # learning_rate=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3UJ5VOrtXvv"
      },
      "outputs": [],
      "source": [
        "from predibase.resource.model import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1H7TOB4NSnQG"
      },
      "outputs": [],
      "source": [
        "# Wait for the job to finish and get training updates and metrics\n",
        "model: Model = job.get()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3HWH_VFiz68"
      },
      "source": [
        "# **Download your fine-tuned LLM**\n",
        "\n",
        "In this quickstart, we're running [adapter-based fine-tuning](https://huggingface.co/docs/peft/conceptual_guides/lora), so the exported model files will contain only the adapter weights, not the full LLM weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj8NNWm8vYbB"
      },
      "outputs": [],
      "source": [
        "model.download(name=\"zephyr_7b_beta_finetuned_gridspace_stanford_harper_valley.zip\", location=\"/Users/myusername/path/to/PredibaseCloud/models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bSLmVloCl6m"
      },
      "source": [
        "# **Prompt your fine-tuned LLM**\n",
        "\n",
        "Predibase supports both real-time inference, as well as batch inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnsNZjkAC1LK"
      },
      "source": [
        "## **Deploy for Real-Time Inference**\n",
        "\n",
        "There are two ways to serve your fine-tuned LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDeiyLWXr894"
      },
      "source": [
        "#### **Real-time inference using _Dynamic Adapter Deployments_** (Recommended)\n",
        "\n",
        "Dynamic adapter deployments allow you to prompt your fine-tuned LLM without needing to create a new deployment for each model you want to prompt. Predibase automatically loads your fine-tuned weights on top of a shared LLM deployment on demand. While this means that there will be a small amount of additional latency, the benefit is that a single LLM deployment can support many different fine-tuned model versions without requiring additional compute.\n",
        "\n",
        "Note: Inference using dynamic adapter deployments is available to both SaaS and VPC users. Predibase provides shared base LLM deployments for use in our SaaS environment. VPC users need deploy their own base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxdoeLSrxz7d"
      },
      "outputs": [],
      "source": [
        "# First, we refresh the base model deployment (e.g., if we restarted the kernel).\n",
        "base_llm_deployment = pc.LLM(uri=f\"pb://deployments/{pb_model_deployment_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3egqqxKxz7d"
      },
      "outputs": [],
      "source": [
        "model_name: str = \"zephyr-7b-beta-gridspace_stanford_harper_valley\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FlG-s9QKRzp-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-j0TCsve9j6X"
      },
      "outputs": [],
      "source": [
        "model: Model = pc.get_model(name=model_name, version=None, model_id=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2zViLX5xz7e"
      },
      "outputs": [],
      "source": [
        "model.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbzN5Kz_wZ7-"
      },
      "outputs": [],
      "source": [
        "# Second, we just specify the adapter to use, which is the model we fine-tuned.\n",
        "adapter_deployment: LLMDeployment = base_llm_deployment.with_adapter(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ogfuPnGxz7e"
      },
      "outputs": [],
      "source": [
        "test_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YTs3W2Rxz7e"
      },
      "outputs": [],
      "source": [
        "result = adapter_deployment.prompt(\n",
        "    data=test_prompt,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    bypass_system_prompt=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE7SlQI3waJf"
      },
      "outputs": [],
      "source": [
        "print(f'\\n[GENERATED_TEXT] FINE_TUNED_MODEL_PREDICTION: \\\"{result.generated_text}\\\"')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}