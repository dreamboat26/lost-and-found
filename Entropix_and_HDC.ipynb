{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csbNOBxAa_X2",
        "outputId": "c4a8f88c-edd9-46b0-ae53-864f519511b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to build vocabulary\n",
        "def build_vocab(dataset, max_vocab_size, stop_words):\n",
        "    counter = Counter()\n",
        "    for item in dataset:\n",
        "        tokens = word_tokenize(item['text'].lower())\n",
        "        tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "        counter.update(tokens)\n",
        "    vocab_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + [token for token, _ in counter.most_common(max_vocab_size)]\n",
        "    return {token: idx for idx, token in enumerate(vocab_tokens)}"
      ],
      "metadata": {
        "id": "puZ_jMNobWA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1vzuS57arnb",
        "outputId": "3d68c93c-91e5-43fc-8e63-b6624ee8f3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.0738\n",
            "Epoch [2/5], Loss: 0.8788\n",
            "Epoch [3/5], Loss: 0.7366\n",
            "Epoch [4/5], Loss: 0.6200\n",
            "Epoch [5/5], Loss: 0.5368\n",
            "Test Accuracy: 62.70%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import random\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Define the Hyperdimensional Computing with Entropix class\n",
        "class HDComputing:\n",
        "    def __init__(self, dim, seed=None, entropy_weight=0.1):\n",
        "        self.dim = dim\n",
        "        self.random_state = np.random.RandomState(seed)\n",
        "        self.entropy_weight = entropy_weight  # Weight for Entropix entropy modulation\n",
        "\n",
        "    def random_hv(self):\n",
        "        return self.random_state.choice([-1, 1], size=self.dim)\n",
        "\n",
        "    def superpose(self, hvs):\n",
        "        sum_hv = np.sum(hvs, axis=0)\n",
        "        return np.sign(sum_hv)\n",
        "\n",
        "    def bind(self, hv1, hv2):\n",
        "        return hv1 * hv2\n",
        "\n",
        "    def permute(self, hv, shifts=1):\n",
        "        return np.roll(hv, shifts)\n",
        "\n",
        "    def apply_entropy(self, hv):\n",
        "        \"\"\"Apply Entropix modulation to enhance reasoning with entropy.\"\"\"\n",
        "        noise = self.random_state.choice([-1, 1], size=self.dim)\n",
        "        return np.sign(hv + self.entropy_weight * noise)\n",
        "\n",
        "# Custom Dataset Class for AG News\n",
        "class AGNewsDataset(Dataset):\n",
        "    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.token_hvs = token_hvs\n",
        "        self.hd = hd\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.stop_words = stop_words\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        text = item['text']\n",
        "        label = item['label']\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n",
        "        tokens = tokens[:self.max_seq_len]\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n",
        "        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Function to create token hypervectors\n",
        "def create_token_hvs(vocab, dim, hd):\n",
        "    return {token: hd.apply_entropy(hd.random_hv()) for token in vocab}\n",
        "\n",
        "# Function to encode sequences with Entropix modulation\n",
        "def encode_sequence(tokens, token_hvs, hd):\n",
        "    sequence_hv = np.zeros(hd.dim)\n",
        "    for i, token in enumerate(tokens):\n",
        "        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n",
        "        permuted_token_hv = hd.permute(token_hv, shifts=i)\n",
        "        sequence_hv += permuted_token_hv\n",
        "    return np.sign(sequence_hv)\n",
        "\n",
        "# HDC Neural Network model with Entropix-enhanced classification\n",
        "class HDCNNClassifier(nn.Module):\n",
        "    def __init__(self, dim, num_classes):\n",
        "        super(HDCNNClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(dim, 512)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        # Apply softmax for classification output\n",
        "        return nn.functional.log_softmax(out, dim=-1)\n",
        "\n",
        "# Main function to train and evaluate the model\n",
        "def main():\n",
        "    dim = 5000\n",
        "    hd = HDComputing(dim, seed=42, entropy_weight=0.2)\n",
        "    max_vocab_size = 5000\n",
        "    max_seq_len = 50\n",
        "    batch_size = 128\n",
        "    num_epochs = 5\n",
        "    learning_rate = 0.001\n",
        "    num_classes = 4\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    dataset = load_dataset('ag_news')\n",
        "    train_data = dataset['train'].shuffle(seed=42)\n",
        "    test_data = dataset['test'].shuffle(seed=42)\n",
        "\n",
        "    vocab = build_vocab(train_data, max_vocab_size, stop_words)\n",
        "    token_hvs = create_token_hvs(vocab, dim, hd)\n",
        "\n",
        "    train_dataset = AGNewsDataset(train_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n",
        "    test_dataset = AGNewsDataset(test_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = HDCNNClassifier(dim, num_classes)\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}')\n",
        "\n",
        "    # Evaluation on test set\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}