{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be-Rtg8l2e56",
        "outputId": "d9039f18-c4b1-450c-b19f-a7c864228b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results from chaotic time computation: [-0.37511952  2.34280433  9.51295896 -2.04751708  4.79721081  6.00045353\n",
            "  9.37811958  8.44037648  2.47638762  8.48944073]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "\n",
        "# Fractal space definition\n",
        "def lorenz(t, state, sigma=10, beta=2.667, rho=28):\n",
        "    \"\"\"\n",
        "    Lorenz attractor dynamics for chaotic behavior.\n",
        "    \"\"\"\n",
        "    x, y, z = state\n",
        "    dx = sigma * (y - x)\n",
        "    dy = x * (rho - z) - y\n",
        "    dz = x * y - beta * z\n",
        "    return [dx, dy, dz]\n",
        "\n",
        "# Fractal computation simulation\n",
        "def fractal_compute(input_data, iterations=1000):\n",
        "    \"\"\"\n",
        "    Perform chaotic computation in fractal space with dynamic parameters.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for i in range(len(input_data)):\n",
        "        # Dynamic parameters for each input\n",
        "        sigma = 10 + np.random.uniform(-2, 2)\n",
        "        beta = 2.667 + np.random.uniform(-0.5, 0.5)\n",
        "        rho = 28 + np.random.uniform(-5, 5)\n",
        "\n",
        "        # Initial state derived from input with noise\n",
        "        initial_state = [input_data[i] + np.random.normal(0, 0.1),\n",
        "                         input_data[i] / 2 + np.random.normal(0, 0.1),\n",
        "                         input_data[i] / 3 + np.random.normal(0, 0.1)]\n",
        "\n",
        "        # Chaotic evolution in fractal space\n",
        "        sol = solve_ivp(\n",
        "            lorenz,\n",
        "            [0, iterations],\n",
        "            initial_state,\n",
        "            t_eval=np.linspace(0, iterations, 100),\n",
        "            args=(sigma, beta, rho)\n",
        "        )\n",
        "        chaotic_result = sol.y[:, -1]  # Final state\n",
        "        results.append(np.mean(chaotic_result))  # Collapse state\n",
        "    return results\n",
        "\n",
        "# Hidden dimension setup\n",
        "def hidden_dimension_compute(input_data):\n",
        "    \"\"\"\n",
        "    Embed computation into a chaotic hidden dimension.\n",
        "    \"\"\"\n",
        "    # Normalize input for fractal space\n",
        "    norm_data = np.tanh(input_data)\n",
        "\n",
        "    # Fractal space computation\n",
        "    fractal_results = fractal_compute(norm_data)\n",
        "\n",
        "    # Collapse fractal results back to standard space\n",
        "    return np.array(fractal_results)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Simulated input data\n",
        "    input_data = np.random.rand(10) * 100  # Random inputs\n",
        "\n",
        "    # Perform hidden dimension computation\n",
        "    results = hidden_dimension_compute(input_data)\n",
        "    print(\"Results from chaotic time computation:\", results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from torch.optim import RAdam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ----- Chaotic Layer Definition -----\n",
        "class ChaoticLayer(nn.Module):\n",
        "    def __init__(self, iterations=500, dt=0.01):\n",
        "        super(ChaoticLayer, self).__init__()\n",
        "        self.iterations = iterations\n",
        "        self.dt = dt\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Lorenz system\n",
        "        def lorenz(t, state, sigma=10, beta=8/3, rho=28):\n",
        "            x, y, z = state\n",
        "            dx = sigma * (y - x)\n",
        "            dy = x * (rho - z) - y\n",
        "            dz = x * y - beta * z\n",
        "            return [dx, dy, dz]\n",
        "\n",
        "        # We'll collect several features from the second half of the trajectory\n",
        "        results = []\n",
        "        for sample in x:\n",
        "            # Initialize with x, y, and R = sqrt(x^2 + y^2)\n",
        "            initial_state = [\n",
        "                sample[0].item(),\n",
        "                sample[1].item(),\n",
        "                np.sqrt(sample[0].item()**2 + sample[1].item()**2),\n",
        "            ]\n",
        "\n",
        "            sol = solve_ivp(\n",
        "                lorenz,\n",
        "                [0, self.iterations * self.dt],\n",
        "                initial_state,\n",
        "                method='RK45',\n",
        "                t_eval=np.linspace(0, self.iterations * self.dt, self.iterations)\n",
        "            )\n",
        "\n",
        "            # Grab second half\n",
        "            halfway = sol.y.shape[1] // 2\n",
        "            x_vals = sol.y[0, halfway:]\n",
        "            y_vals = sol.y[1, halfway:]\n",
        "            z_vals = sol.y[2, halfway:]\n",
        "\n",
        "            # Create multiple chaotic features\n",
        "            mean_x = np.mean(x_vals)\n",
        "            mean_y = np.mean(y_vals)\n",
        "            mean_z = np.mean(z_vals)\n",
        "            std_x  = np.std(x_vals)\n",
        "            std_y  = np.std(y_vals)\n",
        "            std_z  = np.std(z_vals)\n",
        "            mean_r = np.mean(np.sqrt(x_vals**2 + y_vals**2 + z_vals**2))\n",
        "\n",
        "            # You can add or remove any stats you find interesting\n",
        "            chaotic_feature = [\n",
        "                mean_x,\n",
        "                mean_y,\n",
        "                mean_z,\n",
        "                std_x,\n",
        "                std_y,\n",
        "                std_z,\n",
        "                mean_r\n",
        "            ]\n",
        "\n",
        "            results.append(chaotic_feature)\n",
        "\n",
        "        # Return shape: [batch_size, num_chaotic_features]\n",
        "        return torch.tensor(results, dtype=torch.float32)\n",
        "\n",
        "# ----- LCMNet Definition -----\n",
        "class LCMNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LCMNet, self).__init__()\n",
        "\n",
        "        # Let's expand the initial linear layer to produce more hidden units,\n",
        "        # and also collect 7 chaotic features from the ChaoticLayer.\n",
        "        # We'll feed these features into further layers.\n",
        "        self.fc_input = nn.Linear(2, 16)\n",
        "        self.bn_input = nn.BatchNorm1d(16)\n",
        "\n",
        "        self.chaotic = ChaoticLayer(iterations=1000, dt=0.01)\n",
        "\n",
        "        # Suppose the chaotic layer returns 7 features, we can combine those\n",
        "        # with the original hidden representation from fc_input. For example,\n",
        "        # weâ€™ll concatenate them, so we get 16 + 7 = 23 total features.\n",
        "        self.fc_hidden = nn.Linear(16 + 7, 16)\n",
        "        self.bn_hidden = nn.BatchNorm1d(16)\n",
        "\n",
        "        # Final output\n",
        "        self.fc_out = nn.Linear(16, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1) Transform the raw input\n",
        "        x_hidden = self.fc_input(x)\n",
        "        x_hidden = self.relu(self.bn_input(x_hidden))\n",
        "\n",
        "        # 2) Get chaotic features\n",
        "        chaotic_feats = self.chaotic(x)  # shape: [batch_size, 7]\n",
        "\n",
        "        # 3) Concatenate the two representations\n",
        "        combined = torch.cat([x_hidden, chaotic_feats], dim=1)\n",
        "\n",
        "        # 4) Another transform\n",
        "        combined = self.relu(self.bn_hidden(self.fc_hidden(combined)))\n",
        "\n",
        "        # 5) Final linear layer\n",
        "        out = self.fc_out(combined)\n",
        "\n",
        "        # No activation on the final layer (predicting LCM >= 0)\n",
        "        return out\n",
        "\n",
        "# ----- LCM Calculation (Baseline for Comparison) -----\n",
        "def compute_lcm(a, b):\n",
        "    a, b = int(a), int(b)\n",
        "    return abs(a * b) // np.gcd(a, b)\n",
        "\n",
        "# ----- Training and Testing Pipeline -----\n",
        "if __name__ == \"__main__\":\n",
        "    # Synthetic data\n",
        "    num_samples = 200\n",
        "    inputs = torch.randint(1, 50, (num_samples, 2)).float()\n",
        "    lcm_targets = torch.tensor([compute_lcm(a, b) for a, b in inputs.numpy()],\n",
        "                               dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Normalize inputs/targets\n",
        "    input_scaler = MinMaxScaler()\n",
        "    inputs = torch.tensor(input_scaler.fit_transform(inputs), dtype=torch.float32)\n",
        "\n",
        "    target_scaler = MinMaxScaler()\n",
        "    lcm_targets = torch.tensor(target_scaler.fit_transform(lcm_targets),\n",
        "                               dtype=torch.float32)\n",
        "\n",
        "    # Train/test split\n",
        "    train_size = int(0.8 * num_samples)\n",
        "    train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]\n",
        "    train_lcm_targets, test_lcm_targets = lcm_targets[:train_size], lcm_targets[train_size:]\n",
        "\n",
        "    # Model, loss, optimizer\n",
        "    model = LCMNet()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=0.005)\n",
        "\n",
        "    # Optional: try a learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(200):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_inputs)\n",
        "        loss = criterion(outputs, train_lcm_targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = criterion(test_outputs, test_lcm_targets)\n",
        "        print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "\n",
        "        # Inverse transform\n",
        "        test_outputs_orig = target_scaler.inverse_transform(test_outputs.numpy())\n",
        "        test_lcm_targets_orig = target_scaler.inverse_transform(test_lcm_targets.numpy())\n",
        "\n",
        "        print(\"\\nSample Predictions (Original Scale):\")\n",
        "        for i in range(min(10, len(test_inputs))):\n",
        "            print(\n",
        "                f\"Input: {test_inputs[i].numpy()}, \"\n",
        "                f\"Predicted LCM: {test_outputs_orig[i][0]:.2f}, \"\n",
        "                f\"True LCM: {test_lcm_targets_orig[i][0]}\"\n",
        "            )\n",
        "\n",
        "        # MAPE\n",
        "        mape = torch.mean(\n",
        "            torch.abs(\n",
        "                (torch.tensor(test_lcm_targets_orig) - torch.tensor(test_outputs_orig))\n",
        "                / torch.tensor(test_lcm_targets_orig)\n",
        "            )\n",
        "        ) * 100\n",
        "        print(f\"\\nMean Absolute Percentage Error (MAPE): {mape.item():.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM7moIUI4G-8",
        "outputId": "41b894ba-d3bf-43d3-ef17-3c66a4b85715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.3052\n",
            "Epoch 50, Loss: 0.0671\n",
            "Epoch 100, Loss: 0.0174\n",
            "Epoch 150, Loss: 0.0158\n",
            "Test Loss: 0.1351\n",
            "\n",
            "Sample Predictions (Original Scale):\n",
            "Input: [0.39583334 0.8125    ], Predicted LCM: -261.07, True LCM: 40.0\n",
            "Input: [0.45833334 0.375     ], Predicted LCM: -648.91, True LCM: 437.0\n",
            "Input: [0.625     0.5833333], Predicted LCM: -280.49, True LCM: 898.9999389648438\n",
            "Input: [0.9791667 0.       ], Predicted LCM: -1268.66, True LCM: 47.999996185302734\n",
            "Input: [0.22916667 0.1875    ], Predicted LCM: -374.50, True LCM: 60.0\n",
            "Input: [0.0625    0.6458333], Predicted LCM: -290.82, True LCM: 32.0\n",
            "Input: [0.16666667 0.25      ], Predicted LCM: -560.31, True LCM: 117.0\n",
            "Input: [0.33333334 0.22916667], Predicted LCM: -436.20, True LCM: 204.0\n",
            "Input: [0.5833333 0.5833333], Predicted LCM: -327.92, True LCM: 28.999998092651367\n",
            "Input: [0.8125    0.6666667], Predicted LCM: -198.24, True LCM: 1320.0\n",
            "\n",
            "Mean Absolute Percentage Error (MAPE): 518.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from torch.optim import RAdam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ----- Chaotic Layer Definition -----\n",
        "class ChaoticLayer(nn.Module):\n",
        "    def __init__(self, iterations=500, dt=0.01):\n",
        "        super(ChaoticLayer, self).__init__()\n",
        "        self.iterations = iterations\n",
        "        self.dt = dt\n",
        "\n",
        "    def forward(self, x):\n",
        "        def lorenz(t, state, sigma=10, beta=8/3, rho=28):\n",
        "            x, y, z = state\n",
        "            dx = sigma * (y - x)\n",
        "            dy = x * (rho - z) - y\n",
        "            dz = x * y - beta * z\n",
        "            return [dx, dy, dz]\n",
        "\n",
        "        results = []\n",
        "        for sample in x:\n",
        "            initial_state = [\n",
        "                sample[0].item(),\n",
        "                sample[1].item(),\n",
        "                np.sqrt(sample[0].item()**2 + sample[1].item()**2),\n",
        "            ]\n",
        "            sol = solve_ivp(\n",
        "                lorenz,\n",
        "                [0, self.iterations * self.dt],\n",
        "                initial_state,\n",
        "                method='RK45',\n",
        "                t_eval=np.linspace(0, self.iterations * self.dt, self.iterations),\n",
        "            )\n",
        "            x_vals, y_vals, z_vals = sol.y\n",
        "            chaotic_features = [\n",
        "                np.mean(z_vals), np.std(z_vals),\n",
        "                np.mean(np.sqrt(x_vals**2 + y_vals**2 + z_vals**2))\n",
        "            ]\n",
        "            results.append(chaotic_features)\n",
        "\n",
        "        return torch.tensor(results, dtype=torch.float32)\n",
        "\n",
        "# ----- LCMNet Definition -----\n",
        "class LCMNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LCMNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.chaotic = ChaoticLayer(iterations=500)\n",
        "        self.fc2 = nn.Linear(16 + 3, 16)  # Adding chaotic features\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_hidden = self.leaky_relu(self.bn1(self.fc1(x)))\n",
        "        chaotic_features = self.chaotic(x)\n",
        "        combined = torch.cat([x_hidden, chaotic_features], dim=1)\n",
        "        x_out = self.leaky_relu(self.fc2(combined))\n",
        "        x_out = self.fc3(x_out)\n",
        "        return torch.abs(x_out)  # Ensure positive output\n",
        "\n",
        "# ----- LCM Calculation -----\n",
        "def compute_lcm(a, b):\n",
        "    a, b = int(a), int(b)\n",
        "    return abs(a * b) // np.gcd(a, b)\n",
        "\n",
        "# ----- Training and Testing -----\n",
        "if __name__ == \"__main__\":\n",
        "    num_samples = 200\n",
        "    inputs = torch.randint(1, 50, (num_samples, 2)).float()\n",
        "    lcm_targets = torch.tensor([compute_lcm(a, b) for a, b in inputs.numpy()], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    input_scaler = StandardScaler()\n",
        "    inputs = torch.tensor(input_scaler.fit_transform(inputs), dtype=torch.float32)\n",
        "\n",
        "    # Do not scale outputs; let the network learn directly\n",
        "    train_size = int(0.8 * num_samples)\n",
        "    train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]\n",
        "    train_lcm_targets, test_lcm_targets = lcm_targets[:train_size], lcm_targets[train_size:]\n",
        "\n",
        "    model = LCMNet()\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=0.005)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
        "\n",
        "    for epoch in range(1000):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_inputs)\n",
        "        loss = criterion(outputs, train_lcm_targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = criterion(test_outputs, test_lcm_targets)\n",
        "        print(f\"Test Loss: {test_loss.item():.4f}\")\n",
        "\n",
        "        test_outputs_orig = test_outputs.numpy()\n",
        "        test_lcm_targets_orig = test_lcm_targets.numpy()\n",
        "\n",
        "        print(\"\\nSample Predictions:\")\n",
        "        for i in range(10):\n",
        "            print(f\"Input: {test_inputs[i].numpy()}, Predicted: {test_outputs_orig[i][0]:.2f}, True: {test_lcm_targets_orig[i][0]}\")\n",
        "\n",
        "        mape = torch.mean(torch.abs((torch.tensor(test_lcm_targets_orig) - torch.tensor(test_outputs_orig)) / torch.tensor(test_lcm_targets_orig))) * 100\n",
        "        print(f\"\\nMean Absolute Percentage Error (MAPE): {mape.item():.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY7cUM6PW27f",
        "outputId": "3079bcc1-76f6-4a4b-db46-2f215655470b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 355942.9375\n",
            "Epoch 50, Loss: 353984.7500\n",
            "Epoch 100, Loss: 350027.9375\n",
            "Epoch 150, Loss: 343087.4688\n",
            "Epoch 200, Loss: 331731.2812\n",
            "Epoch 250, Loss: 323585.5312\n",
            "Epoch 300, Loss: 313279.0625\n",
            "Epoch 350, Loss: 300450.4062\n",
            "Epoch 400, Loss: 284909.7500\n",
            "Epoch 450, Loss: 275970.5312\n",
            "Epoch 500, Loss: 266125.5938\n",
            "Epoch 550, Loss: 255256.8750\n",
            "Epoch 600, Loss: 243448.4062\n",
            "Epoch 650, Loss: 237267.0312\n",
            "Epoch 700, Loss: 230903.6719\n",
            "Epoch 750, Loss: 224357.7031\n",
            "Epoch 800, Loss: 217664.2500\n",
            "Epoch 850, Loss: 214251.0469\n",
            "Epoch 900, Loss: 210788.0938\n",
            "Epoch 950, Loss: 207276.5781\n",
            "Test Loss: 221388.3750\n",
            "\n",
            "Sample Predictions:\n",
            "Input: [-0.13298798  0.84966195], Predicted: 227.16, True: 828.0\n",
            "Input: [ 0.53011    -0.25693887], Predicted: 196.53, True: 160.0\n",
            "Input: [ 0.45643243 -0.8794018 ], Predicted: 156.89, True: 341.0\n",
            "Input: [0.16172223 0.36552408], Predicted: 214.15, True: 783.0\n",
            "Input: [ 0.53011    -0.11861377], Predicted: 205.19, True: 352.0\n",
            "Input: [0.08804467 0.43468663], Predicted: 214.19, True: 390.0\n",
            "Input: [-0.79608595  1.3337997 ], Predicted: 222.09, True: 602.0\n",
            "Input: [0.53011    0.43468663], Predicted: 239.26, True: 480.0\n",
            "Input: [-0.50137573  1.4721248 ], Predicted: 245.75, True: 90.0\n",
            "Input: [-0.42769817  0.5730117 ], Predicted: 190.74, True: 608.0\n",
            "\n",
            "Mean Absolute Percentage Error (MAPE): 173.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from torch.optim import RAdam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def compute_lcm(a, b):\n",
        "    a, b = int(a), int(b)\n",
        "    return abs(a * b) // np.gcd(a, b)\n",
        "\n",
        "class EnhancedChaoticLayer(nn.Module):\n",
        "    def __init__(self, iterations=500, dt=0.01, system='lorenz'):\n",
        "        super(EnhancedChaoticLayer, self).__init__()\n",
        "        self.iterations = iterations\n",
        "        self.dt = dt\n",
        "        self.system = system\n",
        "\n",
        "    def _lorenz(self, t, state, sigma=10, beta=8/3, rho=28):\n",
        "        x, y, z = state\n",
        "        dx = sigma * (y - x)\n",
        "        dy = x * (rho - z) - y\n",
        "        dz = x * y - beta * z\n",
        "        return [dx, dy, dz]\n",
        "\n",
        "    def _rossler(self, t, state, a=0.2, b=0.2, c=5.7):\n",
        "        x, y, z = state\n",
        "        dx = -y - z\n",
        "        dy = x + a * y\n",
        "        dz = b + z * (x - c)\n",
        "        return [dx, dy, dz]\n",
        "\n",
        "    def _chen(self, t, state, a=35, b=3, c=28):\n",
        "        x, y, z = state\n",
        "        dx = a * (y - x)\n",
        "        dy = (c - a) * x - x * z + c * y\n",
        "        dz = x * y - b * z\n",
        "        return [dx, dy, dz]\n",
        "\n",
        "    def _extract_features(self, trajectory):\n",
        "        x_vals, y_vals, z_vals = trajectory\n",
        "\n",
        "        # Basic statistical features\n",
        "        means = [np.mean(vals) for vals in [x_vals, y_vals, z_vals]]\n",
        "        stds = [np.std(vals) for vals in [x_vals, y_vals, z_vals]]\n",
        "\n",
        "        # Dynamic features\n",
        "        radius = np.sqrt(x_vals**2 + y_vals**2 + z_vals**2)\n",
        "        angular_velocity = np.diff(np.arctan2(y_vals[:-1], x_vals[:-1]))\n",
        "\n",
        "        # Lyapunov-inspired features\n",
        "        divergence_rate = np.mean(np.abs(np.diff(radius)))\n",
        "\n",
        "        features = [\n",
        "            *means, *stds,                      # 6 features\n",
        "            np.mean(radius), np.std(radius),    # 2 features\n",
        "            np.mean(np.abs(angular_velocity)),  # 1 feature\n",
        "            divergence_rate                     # 1 feature\n",
        "        ]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def forward(self, x):\n",
        "        system_func = {\n",
        "            'lorenz': self._lorenz,\n",
        "            'rossler': self._rossler,\n",
        "            'chen': self._chen\n",
        "        }[self.system]\n",
        "\n",
        "        results = []\n",
        "        for sample in x:\n",
        "            initial_state = [\n",
        "                sample[0].item(),\n",
        "                sample[1].item(),\n",
        "                np.sqrt(sample[0].item()**2 + sample[1].item()**2)\n",
        "            ]\n",
        "\n",
        "            sol = solve_ivp(\n",
        "                system_func,\n",
        "                [0, self.iterations * self.dt],\n",
        "                initial_state,\n",
        "                method='RK45',\n",
        "                t_eval=np.linspace(0, self.iterations * self.dt, self.iterations)\n",
        "            )\n",
        "\n",
        "            features = self._extract_features(sol.y)\n",
        "            results.append(features)\n",
        "\n",
        "        return torch.tensor(results, dtype=torch.float32)\n",
        "\n",
        "class EnhancedLCMNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedLCMNet, self).__init__()\n",
        "\n",
        "        # Number of features from chaotic layer (updated)\n",
        "        chaotic_features = 10  # 3 means + 3 stds + 2 radius stats + angular velocity + divergence\n",
        "\n",
        "        # Initial processing\n",
        "        self.fc1 = nn.Linear(2, 32)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "\n",
        "        # Parallel chaotic systems\n",
        "        self.chaotic_lorenz = EnhancedChaoticLayer(iterations=500, system='lorenz')\n",
        "        self.chaotic_rossler = EnhancedChaoticLayer(iterations=500, system='rossler')\n",
        "        self.chaotic_chen = EnhancedChaoticLayer(iterations=500, system='chen')\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = 32 + (chaotic_features * 3)  # Regular features + (3 chaotic systems * features per system)\n",
        "        self.fc2 = nn.Linear(combined_features, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.fc4 = nn.Linear(32, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Regular path\n",
        "        x_hidden = self.leaky_relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        # Chaotic features from different systems\n",
        "        lorenz_features = self.chaotic_lorenz(x)\n",
        "        rossler_features = self.chaotic_rossler(x)\n",
        "        chen_features = self.chaotic_chen(x)\n",
        "\n",
        "        # Combine all features\n",
        "        combined = torch.cat([\n",
        "            x_hidden,\n",
        "            lorenz_features,\n",
        "            rossler_features,\n",
        "            chen_features\n",
        "        ], dim=1)\n",
        "\n",
        "        # Final processing\n",
        "        x_out = self.leaky_relu(self.bn2(self.fc2(combined)))\n",
        "        x_out = self.dropout(x_out)\n",
        "        x_out = self.leaky_relu(self.bn3(self.fc3(x_out)))\n",
        "        x_out = self.fc4(x_out)\n",
        "\n",
        "        return torch.abs(x_out)\n",
        "\n",
        "def train_model(model, train_inputs, train_targets, val_inputs, val_targets, epochs=1000, patience=50):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_inputs)\n",
        "        loss = criterion(outputs, train_targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss = criterion(val_outputs, val_targets)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Data preparation\n",
        "    num_samples = 300  # Increased sample size\n",
        "    inputs = torch.randint(1, 50, (num_samples, 2)).float()\n",
        "    lcm_targets = torch.tensor([compute_lcm(a, b) for a, b in inputs.numpy()], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Scaling\n",
        "    input_scaler = StandardScaler()\n",
        "    inputs = torch.tensor(input_scaler.fit_transform(inputs), dtype=torch.float32)\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.7 * num_samples)\n",
        "    val_size = int(0.15 * num_samples)\n",
        "    train_inputs, val_inputs, test_inputs = inputs[:train_size], inputs[train_size:train_size+val_size], inputs[train_size+val_size:]\n",
        "    train_lcm_targets, val_lcm_targets, test_lcm_targets = lcm_targets[:train_size], lcm_targets[train_size:train_size+val_size], lcm_targets[train_size+val_size:]\n",
        "\n",
        "    # Train model\n",
        "    model = EnhancedLCMNet()\n",
        "    model = train_model(model, train_inputs, train_lcm_targets, val_inputs, val_lcm_targets)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(test_inputs)\n",
        "        test_loss = nn.MSELoss()(test_outputs, test_lcm_targets)\n",
        "        print(f\"\\nTest Loss: {test_loss.item():.4f}\")\n",
        "\n",
        "        test_outputs_np = test_outputs.numpy()\n",
        "        test_targets_np = test_lcm_targets.numpy()\n",
        "\n",
        "        print(\"\\nSample Predictions:\")\n",
        "        for i in range(min(10, len(test_inputs))):\n",
        "            orig_inputs = input_scaler.inverse_transform(test_inputs[i].numpy().reshape(1, -1))[0]\n",
        "            print(f\"Numbers: ({int(orig_inputs[0])}, {int(orig_inputs[1])}), \"\n",
        "                  f\"Predicted LCM: {test_outputs_np[i][0]:.1f}, \"\n",
        "                  f\"True LCM: {test_targets_np[i][0]}\")\n",
        "\n",
        "        mape = np.mean(np.abs((test_targets_np - test_outputs_np) / test_targets_np)) * 100\n",
        "        print(f\"\\nMean Absolute Percentage Error: {mape:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIaUM6vMK8Wa",
        "outputId": "5db1f460-792d-4db6-8b99-96a887147141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 511555.9688, Val Loss: 421606.5625\n",
            "Epoch 50, Train Loss: 511260.7500, Val Loss: 421572.7188\n",
            "Epoch 100, Train Loss: 510993.5625, Val Loss: 421247.4688\n",
            "Epoch 150, Train Loss: 510570.0625, Val Loss: 420860.0000\n",
            "Epoch 200, Train Loss: 510182.7500, Val Loss: 420541.7812\n",
            "Epoch 250, Train Loss: 509931.1875, Val Loss: 420270.2812\n",
            "Epoch 300, Train Loss: 509605.3438, Val Loss: 419992.5312\n",
            "Epoch 350, Train Loss: 509379.8750, Val Loss: 419830.0312\n",
            "Epoch 400, Train Loss: 509160.1562, Val Loss: 419684.0000\n",
            "Epoch 450, Train Loss: 508971.9375, Val Loss: 419548.1875\n",
            "Epoch 500, Train Loss: 508686.5625, Val Loss: 419371.1250\n",
            "Epoch 550, Train Loss: 508480.4062, Val Loss: 419242.5312\n",
            "Early stopping triggered at epoch 594\n",
            "\n",
            "Test Loss: 438579.1250\n",
            "\n",
            "Sample Predictions:\n",
            "Numbers: (45, 14), Predicted LCM: 0.4, True LCM: 630.0\n",
            "Numbers: (5, 22), Predicted LCM: 1.5, True LCM: 66.0\n",
            "Numbers: (6, 10), Predicted LCM: 0.5, True LCM: 77.0\n",
            "Numbers: (17, 9), Predicted LCM: 0.6, True LCM: 153.0\n",
            "Numbers: (19, 1), Predicted LCM: 0.2, True LCM: 19.0\n",
            "Numbers: (23, 31), Predicted LCM: 0.4, True LCM: 713.0\n",
            "Numbers: (40, 29), Predicted LCM: 1.5, True LCM: 1160.0\n",
            "Numbers: (20, 48), Predicted LCM: 4.1, True LCM: 240.0\n",
            "Numbers: (16, 36), Predicted LCM: 4.5, True LCM: 144.0\n",
            "Numbers: (1, 2), Predicted LCM: 0.1, True LCM: 2.0\n",
            "\n",
            "Mean Absolute Percentage Error: 98.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "from torch.optim import RAdam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import math\n",
        "\n",
        "def compute_lcm(a, b):\n",
        "    a, b = int(a), int(b)\n",
        "    return abs(a * b) // np.gcd(a, b)\n",
        "\n",
        "class EnhancedChaoticLayer(nn.Module):\n",
        "    def __init__(self, iterations=200, dt=0.01, system='lorenz'):\n",
        "        super(EnhancedChaoticLayer, self).__init__()\n",
        "        self.iterations = iterations\n",
        "        self.dt = dt\n",
        "        self.system = system\n",
        "\n",
        "    def _lorenz(self, t, state, sigma=10, beta=8/3, rho=28):\n",
        "        x, y, z = state\n",
        "        dx = sigma * (y - x)\n",
        "        dy = x * (rho - z) - y\n",
        "        dz = x * y - beta * z\n",
        "        return [dx, dy, dz]\n",
        "\n",
        "    def _rossler(self, t, state, a=0.2, b=0.2, c=5.7):\n",
        "        x, y, z = state\n",
        "        dx = -y - z\n",
        "        dy = x + a * y\n",
        "        dz = b + z * (x - c)\n",
        "        return [dx, dy, dz]\n",
        "\n",
        "    def _extract_features(self, trajectory):\n",
        "        x_vals, y_vals, z_vals = trajectory\n",
        "\n",
        "        # Basic statistical features\n",
        "        means = [np.mean(vals) for vals in [x_vals, y_vals, z_vals]]\n",
        "        stds = [np.std(vals) for vals in [x_vals, y_vals, z_vals]]\n",
        "\n",
        "        # Dynamic features\n",
        "        radius = np.sqrt(x_vals**2 + y_vals**2 + z_vals**2)\n",
        "\n",
        "        features = [\n",
        "            *means, *stds,                      # 6 features\n",
        "            np.mean(radius), np.std(radius),    # 2 features\n",
        "        ]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def forward(self, x):\n",
        "        system_func = {\n",
        "            'lorenz': self._lorenz,\n",
        "            'rossler': self._rossler\n",
        "        }[self.system]\n",
        "\n",
        "        results = []\n",
        "        for sample in x:\n",
        "            # Scale initial conditions based on input values\n",
        "            scale_factor = math.log(abs(sample[0].item() * sample[1].item()) + 1)\n",
        "            initial_state = [\n",
        "                sample[0].item() * scale_factor,\n",
        "                sample[1].item() * scale_factor,\n",
        "                scale_factor\n",
        "            ]\n",
        "\n",
        "            sol = solve_ivp(\n",
        "                system_func,\n",
        "                [0, self.iterations * self.dt],\n",
        "                initial_state,\n",
        "                method='RK45',\n",
        "                t_eval=np.linspace(0, self.iterations * self.dt, self.iterations)\n",
        "            )\n",
        "\n",
        "            features = self._extract_features(sol.y)\n",
        "            results.append(features)\n",
        "\n",
        "        return torch.tensor(results, dtype=torch.float32)\n",
        "\n",
        "class EnhancedLCMNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedLCMNet, self).__init__()\n",
        "\n",
        "        chaotic_features = 8  # 3 means + 3 stds + 2 radius stats\n",
        "\n",
        "        # Initial processing of raw inputs\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "\n",
        "        # Process mathematical features\n",
        "        self.math_fc = nn.Linear(4, 16)  # For GCD-related features\n",
        "        self.bn_math = nn.BatchNorm1d(16)\n",
        "\n",
        "        # Chaotic systems\n",
        "        self.chaotic_lorenz = EnhancedChaoticLayer(iterations=200, system='lorenz')\n",
        "        self.chaotic_rossler = EnhancedChaoticLayer(iterations=200, system='rossler')\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = 16 + 16 + (chaotic_features * 2)\n",
        "        self.fc2 = nn.Linear(combined_features, 32)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.fc3 = nn.Linear(32, 16)\n",
        "        self.fc4 = nn.Linear(16, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    def compute_math_features(self, x):\n",
        "        # Extract mathematical features that might help with LCM\n",
        "        a = x[:, 0]\n",
        "        b = x[:, 1]\n",
        "        gcd = torch.tensor([math.gcd(int(a[i]), int(b[i])) for i in range(len(a))], dtype=torch.float32)\n",
        "        product = torch.abs(a * b)\n",
        "        max_val = torch.max(torch.abs(x), dim=1)[0]\n",
        "        min_val = torch.min(torch.abs(x), dim=1)[0]\n",
        "\n",
        "        return torch.stack([gcd, product, max_val, min_val], dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Process raw inputs\n",
        "        x_hidden = self.leaky_relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        # Process mathematical features\n",
        "        math_features = self.compute_math_features(x)\n",
        "        math_hidden = self.leaky_relu(self.bn_math(self.math_fc(math_features)))\n",
        "\n",
        "        # Get chaotic features\n",
        "        lorenz_features = self.chaotic_lorenz(x)\n",
        "        rossler_features = self.chaotic_rossler(x)\n",
        "\n",
        "        # Combine all features\n",
        "        combined = torch.cat([\n",
        "            x_hidden,\n",
        "            math_hidden,\n",
        "            lorenz_features,\n",
        "            rossler_features\n",
        "        ], dim=1)\n",
        "\n",
        "        # Final processing\n",
        "        x_out = self.leaky_relu(self.bn2(self.fc2(combined)))\n",
        "        x_out = self.dropout(x_out)\n",
        "        x_out = self.leaky_relu(self.fc3(x_out))\n",
        "        x_out = self.fc4(x_out)\n",
        "\n",
        "        return torch.abs(x_out)\n",
        "\n",
        "def train_model(model, train_inputs, train_targets, val_inputs, val_targets, epochs=1000, patience=50):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = RAdam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20, verbose=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(train_inputs)\n",
        "        loss = criterion(outputs, train_targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss = criterion(val_outputs, val_targets)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "            train_mape = torch.mean(torch.abs((train_targets - outputs) / train_targets)) * 100\n",
        "            val_mape = torch.mean(torch.abs((val_targets - val_outputs) / val_targets)) * 100\n",
        "            print(f\"Epoch {epoch}, Train MAPE: {train_mape:.2f}%, Val MAPE: {val_mape:.2f}%\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch}\")\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Data preparation\n",
        "    num_samples = 500\n",
        "    inputs = torch.randint(1, 50, (num_samples, 2)).float()\n",
        "    lcm_targets = torch.tensor([compute_lcm(a, b) for a, b in inputs.numpy()], dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Log transform the targets\n",
        "    lcm_targets = torch.log1p(lcm_targets)\n",
        "\n",
        "    # Scaling\n",
        "    input_scaler = StandardScaler()\n",
        "    inputs = torch.tensor(input_scaler.fit_transform(inputs), dtype=torch.float32)\n",
        "\n",
        "    # Split data\n",
        "    train_size = int(0.7 * num_samples)\n",
        "    val_size = int(0.15 * num_samples)\n",
        "    train_inputs = inputs[:train_size]\n",
        "    val_inputs = inputs[train_size:train_size+val_size]\n",
        "    test_inputs = inputs[train_size+val_size:]\n",
        "\n",
        "    train_targets = lcm_targets[:train_size]\n",
        "    val_targets = lcm_targets[train_size:train_size+val_size]\n",
        "    test_targets = lcm_targets[train_size+val_size:]\n",
        "\n",
        "    # Train model\n",
        "    model = EnhancedLCMNet()\n",
        "    model = train_model(model, train_inputs, train_targets, val_inputs, val_targets)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(test_inputs)\n",
        "\n",
        "        # Convert back from log space\n",
        "        test_outputs_original = torch.expm1(test_outputs).numpy()\n",
        "        test_targets_original = torch.expm1(test_targets).numpy()\n",
        "\n",
        "        print(\"\\nSample Predictions:\")\n",
        "        for i in range(min(10, len(test_inputs))):\n",
        "            orig_inputs = input_scaler.inverse_transform(test_inputs[i].numpy().reshape(1, -1))[0]\n",
        "            print(f\"Numbers: ({int(orig_inputs[0])}, {int(orig_inputs[1])}), \"\n",
        "                  f\"Predicted LCM: {test_outputs_original[i][0]:.1f}, \"\n",
        "                  f\"True LCM: {test_targets_original[i][0]:.1f}\")\n",
        "\n",
        "        mape = np.mean(np.abs((test_targets_original - test_outputs_original) / test_targets_original)) * 100\n",
        "        print(f\"\\nMean Absolute Percentage Error: {mape:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVUi8zAAf9p4",
        "outputId": "19b2d5e0-1a2e-4104-a3e8-b8c839fcc264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train MAPE: 96.53%, Val MAPE: 92.59%\n",
            "Epoch 50, Train MAPE: 96.44%, Val MAPE: 96.31%\n",
            "Early stopping triggered at epoch 50\n",
            "\n",
            "Sample Predictions:\n",
            "Numbers: (2, 4), Predicted LCM: 0.4, True LCM: 15.0\n",
            "Numbers: (28, 15), Predicted LCM: 0.2, True LCM: 420.0\n",
            "Numbers: (49, 28), Predicted LCM: 0.1, True LCM: 196.0\n",
            "Numbers: (12, 45), Predicted LCM: 0.1, True LCM: 180.0\n",
            "Numbers: (29, 16), Predicted LCM: 0.1, True LCM: 464.0\n",
            "Numbers: (8, 32), Predicted LCM: 0.3, True LCM: 32.0\n",
            "Numbers: (34, 3), Predicted LCM: 0.1, True LCM: 102.0\n",
            "Numbers: (1, 21), Predicted LCM: 0.2, True LCM: 21.0\n",
            "Numbers: (42, 8), Predicted LCM: 0.1, True LCM: 168.0\n",
            "Numbers: (17, 13), Predicted LCM: 0.2, True LCM: 221.0\n",
            "\n",
            "Mean Absolute Percentage Error: 99.56%\n"
          ]
        }
      ]
    }
  ]
}